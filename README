Pre-requisite:

If git is not installed, for linux, install it using:
- sudo yum install git-all

Install a bunch of packages needed for this code.
sudo pip install boto3
sudo pip install mrjob
sudo pip install warc
sudo pip install https://github.com/commoncrawl/gzipstream/archive/master.zip

Eg of running job locally:
python analysis/domain_count.py --no-output --output-dir out input/example.paths.gz

Eg of running job on EMR for testing:
python analysis/domain_count.py -r emr --conf-path analysis/mrjob.conf --no-output --output-dir $OUTPUT_DIR $INPUT_FILEPATH

For full run, change the num of core instances appropriately:
python analysis/domain_count.py -r emr --conf-path analysis/mrjob.conf --num_core_instances=10 --no-output --output-dir $OUTPUT_DIR $INPUT_FILEPATH

Example of $INPUT_FILEPATH is:
s3://commoncrawl/crawl-data/CC-MAIN-2017-04/wet.paths.gz
NOTE: As of 3/4/2017, the gz version might not work due to org.apache.hadoop.mapred.lib.NLineInputFormat not able to deal with compress file.
